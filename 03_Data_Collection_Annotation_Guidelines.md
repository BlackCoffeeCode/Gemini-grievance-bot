### 2. Real-World Data Collection Prompt
If using crowdsourcing or actual grievance data:

ANNOTATION GUIDELINES for Human Annotators:

Task: Label each grievance with appropriate tags

Step 1 - Primary Classification: □ Legitimate □ Spam □ Duplicate

Step 2 - If SPAM, specify type: □ Irrelevant content □ Abusive language □ Promotional □ Test submission □ Gibberish □ Bot-generated

Step 3 - If DUPLICATE, answer:

What is the original grievance ID? ___________

Similarity level? □ Exact □ High □ Moderate □ Low

Is it same person resubmitting? □ Yes □ No

Time gap from original: _____ hours/days

Step 4 - Quality Checks:

Is location mentioned? □ Yes □ No

Is issue clearly described? □ Yes □ No

Is action requested? □ Yes □ No

Language quality: □ Clear □ Moderate □ Poor

Red Flags to mark: □ Contains personal attacks □ Has contact info/URLs □ Excessive capitalization □ Nonsensical content □ Appears automated □ Multiple similar submissions from same user
